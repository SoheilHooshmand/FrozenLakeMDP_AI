{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=1, random_action=False, max_steps=100):\n",
    "    images = []  \n",
    "    done = False\n",
    "    truncated = False\n",
    "    state, info = env.reset()\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "    while not done and not truncated:\n",
    "        i += 1\n",
    "        if i > max_steps:\n",
    "            break\n",
    "        action = np.random.randint(4) if random_action else policy[state]\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        img = env.render()\n",
    "        images.append(img)\n",
    "        if not random_action:\n",
    "            print(f\"action: {action}, state: {state}, reward: {reward}, done: {done}, truncated: {truncated}, info: {info}\")\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(video_path, video_width=500):\n",
    "    video_file = open(video_path, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "total_reward = record_video(env, None, 'frozenlake_random.mp4', fps=3, random_action=True)\n",
    "print(f\"total reward: {total_reward}\")\n",
    "show_video('frozenlake_random.mp4', video_width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FrozenLake MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeMDP:\n",
    "    def __init__(self, is_slippery):\n",
    "        self.is_slippery = is_slippery\n",
    "        self.terminal_states = np.zeros(16, dtype=int)\n",
    "        self.terminal_states[[5, 7, 11, 12, 15]] = 1\n",
    "        self.reward_fn = np.zeros(16, dtype=int)\n",
    "        self.reward_fn[15] = 1\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return self.terminal_states[state]\n",
    "    \n",
    "    def get_reward_function(self):\n",
    "        return self.reward_fn\n",
    "\n",
    "    def next_state_det(self, state, action):\n",
    "        if action == 0:    # LEFT\n",
    "            next_state = state - 1 if state % 4 != 0 else state\n",
    "        elif action == 1:  # DOWN\n",
    "            next_state = state + 4 if state // 4 != 3 else state\n",
    "        elif action == 2:  # RIGHT\n",
    "            next_state = state + 1 if state % 4 != 3 else state\n",
    "        elif action == 3:  # UP\n",
    "            next_state = state - 4 if state // 4 != 0 else state\n",
    "        else:         # WRONG ACTION\n",
    "            next_state = state\n",
    "        return next_state\n",
    "    \n",
    "    def trans_prob(self, state, action):\n",
    "        prob = np.zeros((16,), dtype=float)\n",
    "        if not self.is_slippery:\n",
    "            prob[self.next_state_det(state, action)] = 1.0\n",
    "        else:\n",
    "            prob[self.next_state_det(state, action)] += 1/3\n",
    "            prob[self.next_state_det(state, (action+1)%4)] += 1/3\n",
    "            prob[self.next_state_det(state, (action-1)%4)] += 1/3\n",
    "        return prob\n",
    "\n",
    "    def next_state_reward(self, state, action):\n",
    "        next_state_probs = self.trans_prob(state, action)\n",
    "        next_state = np.random.choice(16, p=next_state_probs)\n",
    "        reward = self.reward_fn[next_state]\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics = FrozenLakeMDP(is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward function of the environent\n",
    "dynamics.get_reward_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating if a given state is a terminal state (= hole or goal)\n",
    "print(dynamics.is_terminal(0), dynamics.is_terminal(7), dynamics.is_terminal(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we take action `a` in state `s`,\n",
    "# what is the probability of landing in each state?\n",
    "dynamics.trans_prob(14, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we take action `a` in state `s`, what do we get?\n",
    "# this is done through sampling the transition probability\n",
    "next_state, reward = dynamics.next_state_reward(14, 2)\n",
    "print(next_state, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(dynamics, policy, gamma=0.9, num_iter=10):\n",
    "    \"\"\"\n",
    "    evaluates policy based on Iterative Policy Evaluation.\n",
    "    Args:\n",
    "        dynamics (FrozenLakeMDP): dynamics of the environment.\n",
    "        policy (numpy.array): policy we want to evaluate.\n",
    "        gamma (float): discount factor.\n",
    "        num_iter (int): number of iterations for the loop.\n",
    " \n",
    "    Returns:\n",
    "        numpy.array: state value function.\n",
    "    \"\"\"\n",
    "    # TODO: implement Iterative Policy Evaluation algorithm\n",
    "\n",
    "    s_value_function = np.zeros(16, dtype=float)\n",
    "    for _ in range(num_iter):\n",
    "        new_value_function = np.zeros_like(s_value_function)\n",
    "\n",
    "        for state in range(16):\n",
    "            if not dynamics.is_terminal(state):\n",
    "                action = policy[state]\n",
    "                action_prob = 1.0  \n",
    "                for next_state in range(16):\n",
    "                    transition_prob = dynamics.trans_prob(state, action)[next_state]\n",
    "                    reward = dynamics.next_state_reward(state, action)[1]\n",
    "                    new_value_function[state] += action_prob * transition_prob * (reward + gamma * s_value_function[next_state])\n",
    "\n",
    "        s_value_function = new_value_function\n",
    "    return s_value_function\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int32' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 2. shortest-path policy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m----> 9\u001b[0m s_value_function \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 24\u001b[0m, in \u001b[0;36mpolicy_evaluation\u001b[1;34m(dynamics, policy, gamma, num_iter)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dynamics\u001b[38;5;241m.\u001b[39mis_terminal(state):\n\u001b[0;32m     23\u001b[0m     expected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action, action_prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m next_state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m     26\u001b[0m             transition_prob \u001b[38;5;241m=\u001b[39m dynamics\u001b[38;5;241m.\u001b[39mtrans_prob(state, action)[next_state]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int32' object is not iterable"
     ]
    }
   ],
   "source": [
    "dynamics = FrozenLakeMDP(is_slippery=False)\n",
    "\n",
    "# 1. go-right policy\n",
    "policy = 2 * np.ones(16, dtype=int)\n",
    "\n",
    "# 2. shortest-path policy\n",
    "policy = np.array([1, 2, 1, 0, 1, -1, 1, -1, 2, 1, 1, -1, -1, 2, 2, -1])\n",
    "\n",
    "s_value_function = policy_evaluation(dynamics, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print and analyze the state value function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_improvement(dynamics, s_value_function, gamma=0.9):\n",
    "    \"\"\"\n",
    "    obtains a policy in a greedy manner based on current state value function.\n",
    " \n",
    "    Args:\n",
    "        dynamics (FrozenLakeMDP): dynamics of the environment.\n",
    "        s_value_function (numpy.array): state value function.\n",
    "        gamma (float): discount factor.\n",
    " \n",
    "    Returns:\n",
    "        numpy.array: the greedy policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement Greedy Policy Improvement algorithm\n",
    "\n",
    "    policy = np.random.randint(0, 4, size=16)  \n",
    "    for state in range(16):\n",
    "        if not dynamics.is_terminal(state):\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "\n",
    "            for action in range(4):\n",
    "                action_value = 0.0\n",
    "\n",
    "                for next_state in range(16):\n",
    "                    transition_prob = dynamics.trans_prob(state, action)[next_state]\n",
    "                    reward = dynamics.next_state_reward(state, action)[1]\n",
    "                    action_value += transition_prob * (reward + gamma * s_value_function[next_state])\n",
    "\n",
    "                if action_value > best_value:\n",
    "                    best_value = action_value\n",
    "                    best_action = action\n",
    "\n",
    "            policy[state] = best_action  \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(dynamics, gamma=0.9, outer_iter=100, inner_iter=100):\n",
    "    \"\"\"\n",
    "    optimizes a policy based on Policy Iteration\n",
    " \n",
    "    Args:\n",
    "        dynamics (FrozenLakeMDP): dynamics of the environment.\n",
    "        gamma (float): discount factor.\n",
    "        outer_iter (int): number of iterations for the Policy Iteration loop.\n",
    "        inner_iter (int): number of iterations for the Policy Evaluation loop.\n",
    " \n",
    "    Returns:\n",
    "        numpy.array: the optimized policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement Policy Iteration algorithm\n",
    "    policy = np.random.randint(0, 4, size=16)\n",
    "    for _ in range(outer_iter):\n",
    "       \n",
    "        s_value_function = policy_evaluation(dynamics, policy, gamma, inner_iter)\n",
    "\n",
    "        \n",
    "        new_policy = greedy_policy_improvement(dynamics, s_value_function, gamma)\n",
    "\n",
    "        \n",
    "        if np.array_equal(new_policy, policy):\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_iteration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: test and analyze the algorithm\u001b[39;00m\n\u001b[0;32m      3\u001b[0m dynamics \u001b[38;5;241m=\u001b[39m FrozenLakeMDP(is_slippery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_iteration\u001b[49m(dynamics)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy_iteration' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: test and analyze the algorithm\n",
    "\n",
    "dynamics = FrozenLakeMDP(is_slippery=False)\n",
    "policy = policy_iteration(dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test the policy on the environment\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "total_reward = record_video(env, policy, 'frozenlake_random.mp4', fps=5, random_action=False)\n",
    "print(f\"total reward: {total_reward}\")\n",
    "show_video('frozenlake_random.mp4', video_width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:  # The Q-Learning RL agent\n",
    "\n",
    "    def __init__(self, num_states, num_actions, epsilon, alpha, gamma=0.9, eps_end=0.01, eps_decay=3e-6):\n",
    "\n",
    "        self.num_states = num_states    # number of possible states\n",
    "        self.num_actions = num_actions  # number of possible actions\n",
    "        self.gamma = gamma              # discount factor\n",
    "        self.epsilon = epsilon          # initial exploration probability\n",
    "        self.alpha = alpha              # step size\n",
    "        self.eps_decay = eps_decay      # linear decay rate of epsilon\n",
    "        self.eps_end = eps_end          # minimum value for epsilon\n",
    "        self.q_table = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        chooses an action in an epsilon-greedy manner.\n",
    "    \n",
    "        Args:\n",
    "            state (int): current state of the agent.\n",
    "    \n",
    "        Returns:\n",
    "            int: the chosen action\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: implement epsilon-greedy action selection\n",
    "        if isinstance(state, tuple):\n",
    "             state = state[0]\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            \n",
    "            action = np.random.randint(0 ,self.num_actions)\n",
    "        else:\n",
    "            \n",
    "            action = np.argmax(self.q_table[state, :])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        updates the q-table based on a single interaction with the environment.\n",
    "    \n",
    "        Args:\n",
    "            state (int): state of the agent.\n",
    "            action (int): action chosen by the agent.\n",
    "            reward (int): reward obtained by the agent.\n",
    "            next_state (int): next state of the agent.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement Q-table update\n",
    "        current_q_value = self.q_table[state, action]\n",
    "        max_next_q_value = np.max(self.q_table[next_state, :])\n",
    "        new_q_value = current_q_value + self.alpha * (reward + self.gamma * max_next_q_value - current_q_value)\n",
    "        self.q_table[state, action] = new_q_value\n",
    "\n",
    "        # epsilon decay\n",
    "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > self.eps_end else self.eps_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=100000):\n",
    "    \"\"\"\n",
    "        trains an agent through interactions with the environemnt using Q-learning.\n",
    "    \n",
    "        Args:\n",
    "            env (gym.Env): the gym environment.\n",
    "            agent (QAgent): the Q-learning agent.\n",
    "            n_episodes (int): number of training episodes.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "\n",
    "        # TODO: implement the training loop for Q-learning\n",
    "\n",
    "        state = env.reset()  \n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            action = agent.choose_action(state)\n",
    "\n",
    "            \n",
    "            next_state, reward, done, info, *extra_values = env.step(action)\n",
    "\n",
    "            \n",
    "            if isinstance(state, tuple):\n",
    "                agent.learn(state[0], action, reward, next_state)\n",
    "            if isinstance(state, int):\n",
    "                agent.learn(state, action, reward, next_state)\n",
    "\n",
    "           \n",
    "            total_reward += reward\n",
    "\n",
    "           \n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        \n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "agent = QAgent(num_states=16, num_actions=4, epsilon=1.0, alpha=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: obtain the policy by a simple argmax on agent's Q-table\n",
    "policy = np.argmax(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test the policy on the environment\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "total_reward = record_video(env, policy, 'frozenlake_random.mp4', fps=5, random_action=False)\n",
    "print(f\"total reward: {total_reward}\")\n",
    "show_video('frozenlake_random.mp4', video_width=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
